{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngram, BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력된 텍스트로부터 Ngram 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a')\n",
      "('is', 'a', 'foo')\n",
      "('a', 'foo', 'bar')\n",
      "('foo', 'bar', 'sentences')\n",
      "('bar', 'sentences', 'and')\n",
      "('sentences', 'and', 'i')\n",
      "('and', 'i', 'want')\n",
      "('i', 'want', 'to')\n",
      "('want', 'to', 'ngramize')\n",
      "('to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "from nltk import ngrams\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "n = 3 # you can give 4, 5, 1 or any number less than sentences length\n",
    "ngramsres = ngrams(sentence.split(), n)\n",
    "for grams in ngramsres:\n",
    "    print (grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 많이 출현한 바이그램 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Allon', 'Bacuth'),\n",
       " ('Ashteroth', 'Karnaim'),\n",
       " ('Ben', 'Ammi'),\n",
       " ('En', 'Mishpat'),\n",
       " ('Jegar', 'Sahadutha'),\n",
       " ('Salt', 'Sea'),\n",
       " ('Whoever', 'sheds'),\n",
       " ('appoint', 'overseers'),\n",
       " ('aromatic', 'resin'),\n",
       " ('cutting', 'instrument')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "    nltk.corpus.genesis.words('english-web.txt'))\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력된 텍스트로부터 바이그램 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 'I'),\n",
       " ('I', 'am'),\n",
       " ('I', 'do'),\n",
       " ('Sam', 'I'),\n",
       " ('am', '!'),\n",
       " ('and', 'ham'),\n",
       " ('do', 'not'),\n",
       " ('eggs', 'and'),\n",
       " ('green', 'eggs'),\n",
       " ('ham', ','),\n",
       " ('like', 'green'),\n",
       " ('like', 'them'),\n",
       " ('not', 'like'),\n",
       " ('them', 'Sam')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "text = \"I do not like green eggs and ham, I do not like them Sam I am!\"\n",
    "tokens = nltk.wordpunct_tokenize(text)\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "sorted(bigram for bigram, score in scored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "바이그램의 빈도수 세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', 'and'), 1271),\n",
       " ((',', '\"'), 510),\n",
       " (('of', 'the'), 369),\n",
       " ((\"'\", 's'), 300),\n",
       " (('in', 'the'), 270),\n",
       " (('said', ','), 258),\n",
       " (('said', 'to'), 197),\n",
       " (('.', 'He'), 174),\n",
       " (('the', 'land'), 161),\n",
       " (('.', 'The'), 153)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(finder.nbest(trigram_measures.raw_freq, 2))\n",
    "sorted(finder.ngram_fd.items(), key=lambda t: (-t[1], t[0]))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW(Bag of Words, 단어가방)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit-learn의 CounterVectorizer 활용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer`는 다음과 같은 세가지 작업을 수행한다.\n",
    "\n",
    "1. 문서를 토큰 리스트로 변환한다.\n",
    "2. 각 문서에서 토큰의 출현 빈도를 센다.\n",
    "3. 각 문서를 BOW 인코딩 벡터로 변환한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트를 입력하고 단어가방(BOW)에 넣기\n",
    "\n",
    "단어 수만큼 벡터를 만들고, 각 단어를 벡터에 abc 순으로 넣기\n",
    "\n",
    "ex) and는 0번가방에, document는 1번가방에 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'document': 1,\n",
       " 'first': 2,\n",
       " 'is': 3,\n",
       " 'last': 4,\n",
       " 'one': 5,\n",
       " 'second': 6,\n",
       " 'the': 7,\n",
       " 'third': 8,\n",
       " 'this': 9}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',    \n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가방에 들어간 단어들을 출력하고, 각 문장에서 그 단어가 출현한 빈도수 세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 0, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "vect.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words(불용어) 설정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Words 는 문서에서 단어장을 생성할 때 무시할 수 있는 단어를 말한다. 보통 영어의 관사나 접속사, 한국어의 조사 등이 여기에 해당한다. `stop_words` 인수로 조절할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'first': 1, 'one': 2, 'second': 3, 'third': 4}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Words(불용어)를 제외하고, 벡터를 다시 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0],\n",
       "       [2, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 1],\n",
       "       [1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF(Term Frequency – Inverse Document Frequency) 인코딩은 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소하는 방법이다. \n",
    "\n",
    "\n",
    "구제적으로는 문서 $d$(document)와 단어 $t$ 에 대해 다음과 같이 계산한다.\n",
    "\n",
    "$$ \\text{tf-idf}(d, t) = \\text{tf}(d, t) \\cdot \\text{idf}(t) $$\n",
    "\n",
    "\n",
    "여기에서\n",
    "\n",
    "* $\\text{tf}(d, t)$: term frequency. 특정한 단어의 빈도수\n",
    "* $\\text{idf}(t)$ : inverse document frequency. 특정한 단어가 들어 있는 문서의 수에 반비례하는 수\n",
    " \n",
    " $$ \\text{idf}(d, t) = \\log \\dfrac{n}{1 + \\text{df}(t)} $$\n",
    " \n",
    "* $n$ : 전체 문서의 수\n",
    "* $\\text{df}(t)$:  단어 $t$를 가진 문서의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.46979139,  0.58028582,  0.38408524,  0.        ,\n",
       "         0.        ,  0.38408524,  0.        ,  0.38408524],\n",
       "       [ 0.        ,  0.6876236 ,  0.        ,  0.28108867,  0.        ,\n",
       "         0.53864762,  0.28108867,  0.        ,  0.28108867],\n",
       "       [ 0.51184851,  0.        ,  0.        ,  0.26710379,  0.51184851,\n",
       "         0.        ,  0.26710379,  0.51184851,  0.26710379],\n",
       "       [ 0.        ,  0.46979139,  0.58028582,  0.38408524,  0.        ,\n",
       "         0.        ,  0.38408524,  0.        ,  0.38408524]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidv = TfidfVectorizer().fit(corpus)\n",
    "tfidv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 활용 사례"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 Scikit-Learn의 문자열 분석기를 사용하여 웹사이트에 특정한 단어가 어느 정도 사용되었는지 빈도수를 알아보는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Invalid control character at: line 63 column 10 (char 1291)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-949b34004ba0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://github.com/ahhn/oss/raw/master/resources/Ngram_BOW_TF-IDF.ipynb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mjson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cells\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cell_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"markdown\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhannanum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnumeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Invalid control character at: line 63 column 10 (char 1291)"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "import string\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "\n",
    "#f = urlopen(\"https://www.datascienceschool.net/download-notebook/708e711429a646818b9dcbb581e0c10a/\")\n",
    "f = urlopen(\"https://github.com/ahhn/oss/raw/master/resources/Ngram_BOW_TF-IDF.ipynb\")\n",
    "\n",
    "json = json.loads(f.read())\n",
    "cell = [\"\\n\".join(c[\"source\"]) for c in json[\"cells\"] if c[\"cell_type\"] == \"markdown\"]\n",
    "docs = [w for w in hannanum.nouns(\" \".join(cell)) if ((not w[0].isnumeric()) and (w[0] not in string.punctuation))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서는 하나의 문서가 하나의 단어로만 이루어져 있다. 따라서 `CountVectorizer`로 이 문서 집합을 처리하면  각 문서는 하나의 원소만 1이고 나머지 원소는 0인 벡터가 된다. 이 벡터의 합으로 빈도를 알아보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aa52ecf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "vect = CountVectorizer().fit(docs)\n",
    "count = vect.transform(docs).toarray().sum(axis=0)\n",
    "idx = np.argsort(-count)\n",
    "count = count[idx]\n",
    "feature_name = np.array(vect.get_feature_names())[idx]\n",
    "plt.bar(range(len(count)), count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('단어', 12),\n",
      " ('문서', 6),\n",
      " ('벡터', 6),\n",
      " ('생성', 4),\n",
      " ('빈도수', 4),\n",
      " ('텍스트', 3),\n",
      " ('출현', 3),\n",
      " ('바이그램', 3),\n",
      " ('다음', 3),\n",
      " ('하나', 3)]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(zip(feature_name, count))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
