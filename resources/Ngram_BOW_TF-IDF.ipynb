{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngram, BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력된 텍스트로부터 Ngram 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a')\n",
      "('is', 'a', 'foo')\n",
      "('a', 'foo', 'bar')\n",
      "('foo', 'bar', 'sentences')\n",
      "('bar', 'sentences', 'and')\n",
      "('sentences', 'and', 'i')\n",
      "('and', 'i', 'want')\n",
      "('i', 'want', 'to')\n",
      "('want', 'to', 'ngramize')\n",
      "('to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "from nltk import ngrams\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "n = 3 # you can give 4, 5, 1 or any number less than sentences length\n",
    "ngramsres = ngrams(sentence.split(), n)\n",
    "for grams in ngramsres:\n",
    "    print (grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 많이 출현한 바이그램 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Allon', 'Bacuth'),\n",
       " ('Ashteroth', 'Karnaim'),\n",
       " ('Ben', 'Ammi'),\n",
       " ('En', 'Mishpat'),\n",
       " ('Jegar', 'Sahadutha'),\n",
       " ('Salt', 'Sea'),\n",
       " ('Whoever', 'sheds'),\n",
       " ('appoint', 'overseers'),\n",
       " ('aromatic', 'resin'),\n",
       " ('cutting', 'instrument')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "    nltk.corpus.genesis.words('english-web.txt'))\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력된 텍스트로부터 바이그램 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 'I'),\n",
       " ('I', 'am'),\n",
       " ('I', 'do'),\n",
       " ('Sam', 'I'),\n",
       " ('am', '!'),\n",
       " ('and', 'ham'),\n",
       " ('do', 'not'),\n",
       " ('eggs', 'and'),\n",
       " ('green', 'eggs'),\n",
       " ('ham', ','),\n",
       " ('like', 'green'),\n",
       " ('like', 'them'),\n",
       " ('not', 'like'),\n",
       " ('them', 'Sam')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "text = \"I do not like green eggs and ham, I do not like them Sam I am!\"\n",
    "tokens = nltk.wordpunct_tokenize(text)\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "sorted(bigram for bigram, score in scored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "바이그램의 빈도수 세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', 'and'), 1271),\n",
       " ((',', '\"'), 510),\n",
       " (('of', 'the'), 369),\n",
       " ((\"'\", 's'), 300),\n",
       " (('in', 'the'), 270),\n",
       " (('said', ','), 258),\n",
       " (('said', 'to'), 197),\n",
       " (('.', 'He'), 174),\n",
       " (('the', 'land'), 161),\n",
       " (('.', 'The'), 153)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(finder.nbest(trigram_measures.raw_freq, 2))\n",
    "sorted(finder.ngram_fd.items(), key=lambda t: (-t[1], t[0]))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW(Bag of Words, 단어가방)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit-learn의 CounterVectorizer 활용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer`는 다음과 같은 세가지 작업을 수행한다.\n",
    "\n",
    "1. 문서를 토큰 리스트로 변환한다.\n",
    "2. 각 문서에서 토큰의 출현 빈도를 센다.\n",
    "3. 각 문서를 BOW 인코딩 벡터로 변환한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트를 입력하고 단어가방(BOW)에 넣기\n",
    "\n",
    "단어 수만큼 벡터를 만들고, 각 단어를 벡터에 abc 순으로 넣기\n",
    "\n",
    "ex) and는 0번가방에, document는 1번가방에 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'document': 1,\n",
       " 'first': 2,\n",
       " 'is': 3,\n",
       " 'last': 4,\n",
       " 'one': 5,\n",
       " 'second': 6,\n",
       " 'the': 7,\n",
       " 'third': 8,\n",
       " 'this': 9}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',    \n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가방에 들어간 단어들을 출력하고, 각 문장에서 그 단어가 출현한 빈도수 세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 0, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "vect.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words(불용어) 설정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Words 는 문서에서 단어장을 생성할 때 무시할 수 있는 단어를 말한다. 보통 영어의 관사나 접속사, 한국어의 조사 등이 여기에 해당한다. `stop_words` 인수로 조절할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'first': 1, 'one': 2, 'second': 3, 'third': 4}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Words(불용어)를 제외하고, 벡터를 다시 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0],\n",
       "       [2, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 1],\n",
       "       [1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF(Term Frequency – Inverse Document Frequency) 인코딩은 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소하는 방법이다. \n",
    "\n",
    "\n",
    "구제적으로는 문서 $d$(document)와 단어 $t$ 에 대해 다음과 같이 계산한다.\n",
    "\n",
    "$$ \\text{tf-idf}(d, t) = \\text{tf}(d, t) \\cdot \\text{idf}(t) $$\n",
    "\n",
    "\n",
    "여기에서\n",
    "\n",
    "* $\\text{tf}(d, t)$: term frequency. 특정한 단어의 빈도수\n",
    "* $\\text{idf}(t)$ : inverse document frequency. 특정한 단어가 들어 있는 문서의 수에 반비례하는 수\n",
    " \n",
    " $$ \\text{idf}(d, t) = \\log \\dfrac{n}{1 + \\text{df}(t)} $$\n",
    " \n",
    "* $n$ : 전체 문서의 수\n",
    "* $\\text{df}(t)$:  단어 $t$를 가진 문서의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.46979139,  0.58028582,  0.38408524,  0.        ,\n",
       "         0.        ,  0.38408524,  0.        ,  0.38408524],\n",
       "       [ 0.        ,  0.6876236 ,  0.        ,  0.28108867,  0.        ,\n",
       "         0.53864762,  0.28108867,  0.        ,  0.28108867],\n",
       "       [ 0.51184851,  0.        ,  0.        ,  0.26710379,  0.51184851,\n",
       "         0.        ,  0.26710379,  0.51184851,  0.26710379],\n",
       "       [ 0.        ,  0.46979139,  0.58028582,  0.38408524,  0.        ,\n",
       "         0.        ,  0.38408524,  0.        ,  0.38408524]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidv = TfidfVectorizer().fit(corpus)\n",
    "tfidv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 활용 사례"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 Scikit-Learn의 문자열 분석기를 사용하여 웹사이트에 특정한 단어가 어느 정도 사용되었는지 빈도수를 알아보는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-63a372e7d48a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://github.com/ahhn/oss/raw/master/resources/sample.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mjson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cells\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cell_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"markdown\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhannanum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnumeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "import string\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "\n",
    "#f = urlopen(\"https://www.datascienceschool.net/download-notebook/708e711429a646818b9dcbb581e0c10a/\")\n",
    "f = urlopen(\"https://github.com/ahhn/oss/raw/master/resources/sample.json\")\n",
    "\n",
    "json = json.loads(f.read())\n",
    "cell = [\"\\n\".join(c[\"source\"]) for c in json[\"cells\"] if c[\"cell_type\"] == \"markdown\"]\n",
    "docs = [w for w in hannanum.nouns(\" \".join(cell)) if ((not w[0].isnumeric()) and (w[0] not in string.punctuation))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서는 하나의 문서가 하나의 단어로만 이루어져 있다. 따라서 `CountVectorizer`로 이 문서 집합을 처리하면  각 문서는 하나의 원소만 1이고 나머지 원소는 0인 벡터가 된다. 이 벡터의 합으로 빈도를 알아보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aa8eeac18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "vect = CountVectorizer().fit(docs)\n",
    "count = vect.transform(docs).toarray().sum(axis=0)\n",
    "idx = np.argsort(-count)\n",
    "count = count[idx]\n",
    "feature_name = np.array(vect.get_feature_names())[idx]\n",
    "plt.bar(range(len(count)), count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('컨테이너', 81),\n",
      " ('도커', 41),\n",
      " ('명령', 34),\n",
      " ('이미지', 33),\n",
      " ('사용', 26),\n",
      " ('가동', 14),\n",
      " ('중지', 13),\n",
      " ('mingw64', 13),\n",
      " ('삭제', 12),\n",
      " ('아이디', 11)]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(zip(feature_name, count))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
